\documentclass{article}

% Pakete einbinden
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{minted}

% Titel, Autor und Datum festlegen
\title{Titel des Papers}
\author{Autor}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
    Die Art und Weise, wie auf Daten zugegriffen wird,
    kann einen erheblichen Einfluss auf die Performance eines Algorithmus haben.
    Insbesondere wenn große Datenmengen häufig zwischen langsamen Speichermedien und dem Prozessor ausgetauscht werden,
    spricht man häufig von einem Engpass des Speicherinterfaces.
    Durch effiziente Datenzugriffe und optimale Nutzung des Caches 
    kann nicht nur die Performance signifikant gesteigert werden, 
    sondern es wird auch sichergestellt, 
    dass die Leistungsfähigkeit selbst bei der Verarbeitung großer Datenmengen stabil bleibt.
    In diesem Paper werden Methoden wie Loop Unrolling, Loop Fusion, Blocking analysiert.
    Um die Effektivität mancher Methoden zu demonstrieren, werden diese gebenchmarkt und mit den Standardmethoden verglichen.
\end{abstract}

\section{Einleitung}
    Im letzten Jahrzehnt hat sich die Rechenleistung von Prozessoren erheblich gesteigert. 
    In Abbildung 1 und 1.1 ist zu erkennen, dass seit der Einführung der 4th Generation im Jahr 2013 sich die Anzahl der Kerne bei Intel, 
    die für den allgemeinen Verbrauchermarkt verfügbar sind, von 6 auf 24 Kernen erhöht hat.
    Es ist wichtig zu beachten, dass es zwar Prozessoren mit einer noch höheren Anzahl von Kernen gibt, 
    diese jedoch in der Regel nicht für den Standard-Endverbraucher bestimmt sind. 
    Ebenso zeigt sich in den Abbildungen 2 und 2.1 ein Anstieg der maximalen Taktfrequenz über die Jahre. 
    Wenn man nun die theoretische maximale Rechenleistung, 
    definiert als:\\ $\textnormal{P}_{\textnormal{max}} = \text{Anzahl der Kerne} \times \text{Turbo Taktfrequenz} \times \text{Flops pro Taktzyklus}$,\\und
    der Entwicklung der Speicherbandbreite gegenüberstellt, wird in Abbildung 3 deutlich, 
    dass die Zunahme der Speicherbandbreite nicht im gleichen Maße wie die Rechenleistung ansteigt. 
    Im Detail hat sich die Bandbreite von 51.2 $\frac{\text{GByte}}{\text{s}}$ im Jahr 2013 auf 89.6 $\frac{\text{GByte}}{\text{s}}$ im Jahr 2024 erhöht. 
    Parallel dazu ist die Performance im gleichen Zeitraum von 187.2 $\frac{\text{Flops}}{\text{s}}$ auf 1945.6 $\frac{\text{Flops}}{\text{s}}$ gestiegen. 
    Diese Diskrepanz zwischen der gesteigerten Rechenleistung und der vergleichsweise langsamer wachsenden Speicherbandbreite 
    verdeutlicht die Notwendigkeit einer Optimierung von Datenzugriffen. 
    Ein effizienter Einsatz des Caches ist dabei unerlässlich, um die Leistung zu maximieren.

    \section{Berechnung der Performancegrenzen}
    In den nachfolgenden Unterabschnitten werden die Formeln vorgestellt, 
    die zur Bewertung von loop-basiertem Code und zur Berechnung der Performancegrenzen herangezogen werden. 
    Es ist jedoch wichtig zu berücksichtigen, dass diese Formeln lediglich eine Annäherung darstellen 
    und nur unter bestimmten Bedingungen gültig sind. Beispielsweise wird vorausgesetzt, 
    dass alle Ressourcen vollständig ausgeschöpft werden die die CPU zu bieten hat. 
    Zudem basieren die Formeln auf Aspekten des idealen Cache-Modells, 
    wie einem unendlich schnellen Cache und der Nichtexistenz von Latenzzeiten.


    \subsection{Maschinenbalance}

    Die Maschinenbalance $\textnormal{B}_{\textnormal{m}}$ ist das Verhältnis 
    aus der maximalen Bandbreite und der theoretischen Rechenleistung
    $\textnormal{B}_{\textnormal{m}} = \frac{\textnormal{B}_{\textnormal{max}}}{\textnormal{P}_{\textnormal{max}}}$.
    Sie beschreibt, wie viele Daten pro Flop übertragen werden können. Zur Veranschaulichung 
    betrachten wir die $\textnormal{B}_{\textnormal{m}}$ für den i7-9700K Prozessor,     % Hier muss ein Quellenverweis hin
    der 2018 erschienen ist und eine maximale Bandbreite von 41.6\,$\frac{\text{GByte}}{\text{s}}$ sowie
    eine theoretische Rechenleistung von 8\,Kerne $\times$ 4.9\,GHz $\times$ 16\,$\frac{\text{Flops}}{\text{Taktzyklus}}$ = 627,2\,$\frac{\text{Flops}}{\text{s}}$ besitzt.
    Die Bandbreite muss noch in Fließkommazahlen umgerechnet werden, die pro Sekunde geladen werden können. Also
    $\frac{41.6 \, \text{GB/s}}{8 \, \text{Byte}} = 5.2 \, \frac{\text{Fließkommazahlen}}{\text{s}}$.
    Daraus ergibt sich, dass $\textnormal{B}_{\textnormal{m}} = \frac{5.2 \, \textnormal{Fließkommazahlen/s}}{627.2 \, \textnormal{Flops/s}} \approx 0.008\,\frac{\textnormal{Fließkommazahlen}}{\textnormal{Flop}}$
    geladen werden können. Mit anderen Worten, bis eine Fließkommazahl geladen ist, müssen
    125 Flops auf einem Wert durchgeführt werden, bis der nächste Wert geladen ist. 
    Die Maschinenbalance bei den neuesten Prozessoren wie dem i9-14900KS liegt bei $\approx 0.0058$, und somit wären 
    172 Flops pro geladene Fließkommazahl erforderlich. Man könnte daher schlussfolgern, 
    dass das Rechnen quasi kostenlos ist und das Laden der Werte den limitierende Faktor darstellt.

    \subsection{Codebalance}
    Die Codebalance $\textnormal{B}_{\textnormal{c}} = \frac{\textnormal{Datenverkehr}}{\textnormal{Flops}}$ ist das Verhältnis
    der zu ladenden und speichernden Fließkommazahlen und der Anzahl der Flops in einer loop Iteration. 
    Hierbei zählt man aber nur die load und store Operationen, welche wirklich über den langsamen Datenpfad verläuft, daher wird $\text{l\_i}$ nicht mitgezählt,
    weil es sich im Register befindet.
    In Abbildung 4 ist ein solcher loop dargestellt, % Hier muss ein Quellenverweis hin
    welcher 4 Flops ausführt und 3 Elemente aus den drei Arrays $\textnormal{X}$, $\textnormal{Y}$ und $\textnormal{Z}$ lädt 
    und eine speicher Operation in $\textnormal{Z}$ durchführt. Die Codebalance ist also $\textnormal{B}_{\textnormal{c}} = \frac{3+1}{4} = 1$. 


    \subsection{Berechnung der zu erwartenden Performance}
    Um die maximal erreichbare Performance eines loop-basierten Codes zu berechnen, 
    bestimmt man den Anteil der maximalen CPU-Performance, die tatsächlich erreicht werden kann. 
    Dieser Anteil wird als ,,lightspeed" $\text{l}$ bezeichnet und ist definiert als 
    $\text{l} = \min(1, \frac{\text{B}_{m}}{\text{B}_{c}})$. 
    Da diese Formel auf den ersten Blick nicht intuitiv ist, 
    kann ein Beispiel zur Veranschaulichung dienen. 
    Betrachten wir erneut den Code aus Abbildung 4. 
    Die Maschinenbalance $\textnormal{B}_{\textnormal{m}}$ 
    beträgt 0.5 $\frac{\textnormal{Fließkommazahlen}}{\textnormal{Flop}}$, 
    während die Codebalance $\textnormal{B}_{\textnormal{c}}$ 1 beträgt, 
    da vier Fließkommazahlen geladen und gespeichert werden müssen und vier Flops ausgeführt werden. 
    Da das Bereitstellen und Abspeichern von Fließkommazahlen nicht so schnell ist wie das Rechnen 
    (was durch $\textnormal{B}_{\textnormal{m}}$  = 0.5 ausgedrückt wird), kann man in der Zeit, 
    in der man vier Flops ausführen würde, 
    nur halb so viele, also 4 $\times$ 0.5 = 2 Fließkommazahlen laden und/oder abspeichern.
    Daher ist die maximale Performance, die erreicht werden kann, 
    $\text{l} = \min(1, \frac{0.5}{1}) = 0.5$. 
    
    Oder anders betrachtet, es könnte einen Code geben,
    der doppelt so viele Flops ausführt wie er Fließkommazahlen lädt und speichert. 
    Dies würde zu einer Codebalance von 0.5 führen. 
    In einem solchen Szenario wäre die Anzahl 
    der zu ladenden und speichernden Fließkommazahlen genau gleich der theoretisch möglichen Anzahl
    an Fließkommazahlen, die geladen und gespeichert werden könnten. 
    Somit würde der maximal zu erreichende Anteil an 
    $\textnormal{P}_{\textnormal{max}}$ bei $\text{l} = \min(1, \frac{0.5}{0.5}) = 1$ liegen.  % Hier muss ein Quellenverweis hin
    Wenn dieser Wert nah an 1 liegt ist man nicht Speichergebunden.
    Die maximal erreichbare Performance ist somit $\textnormal{P} =  \textnormal{l} \times \textnormal{P}_{\textnormal{max}}$ oder 
    $\textnormal{P} =  \min(\textnormal{P}_{\textnormal{max}}, \frac{\textnormal{b}_{\textnormal{max}}}{\textnormal{B}_{\textnormal{c}}})$.

\section{Loop Fusion} 
    Die erste Methode, die zur Optimierung des Zugriffsverhaltens vorgestellt 
    wird und bei der anhand des präsentierten Modells eine Verbesserung erkennbar ist, 
    ist Loop Fusion. Es ist wichtig zu verstehen, dass der Cache ein kleiner, 
    schneller Speicher ist, der die Least Recent Used (LRU) Policy verwendet, um zu entscheiden, 
    welche Daten im Cache verbleiben. Dies bedeutet, 
    dass die am längsten nicht genutzten Daten aus dem Cache entfernt werden, 
    wenn eine neue Cache Line geladen werden muss und kein Platz mehr im Cache vorhanden ist. 
    Dies beschreibt die zeitliche Lokalität. 
    Wenn eine Cache Line geladen wird und die umliegenden Daten ebenfalls in den Cache geladen werden, 
    spricht man von räumlicher Lokalität, da man davon ausgeht, 
    dass diese Daten in naher Zukunft ebenfalls genutzt werden. 
    Typischerweise beträgt die Größe einer Cache-Zeile 64 Bytes, 
    kann aber durch die Befehle \texttt{getconf LEVEL1\_DCACHE\_LINESIZE}, 
    \texttt{getconf LEVEL2\_CACHE\_LINESIZE}, \texttt{getconf LEVEL3\_CACHE\_LINESIZE} 
    leicht bestimmt werden. Um die Größe des Caches zu ermitteln, 
    kann man den Befehl \texttt{lstopo} verwenden, sofern installiert. 
    Betrachtet man Abbildung 5, ist % hier muss ein Quellenverweis hin
    leicht zu erkennen, dass zunächst in einer for-Schleife die Elemente aus dem Array 
    $\textnormal{A}$ mit dem Skalar $\textnormal{2.0}$ 
    multipliziert und in das Array $\textnormal{Z}$ gespeichert werden. 
    Anschließend wird eine weitere for-Schleife ausgeführt, die das Gleiche tut, 
    nur dass hier das Ergebnis in $\textnormal{Y}$ gespeichert wird. 
    Wenn die Arrays zu groß sind und am Ende der ersten for-Schleife die Cache Line, 
    die die ersten Elemente aus A enthält verdrängt wurde, 
    müssen diese für die zweite for-Schleife erneut in den Cache geladen werden. 
    Daten, die in den Cache geladen und aus Platzgründen wieder verdrängt wurden, 
    aber später wieder benötigt und erneut in den Cache geladen werden, nennt man 
    \texttt{Capacity Miss}. Das erstmalige Laden von Daten über langsame Datenpfade wird als 
    \texttt{Cold Miss} bezeichnet. Die Code-Balance $\textnormal{B}_{\textnormal{c}}$ 
    der beiden for-Schleifen beträgt $\frac{2}{1} = 2$. Wenn man jedoch die beiden 
    for-Schleifen zusammenführt (siehe Abbildung 6), spart man sich einen langsamen Load, % hier muss ein Quellenverweis hin
    da der Wert von A[i] in der Zeile zuvor in den Cache geladen wurde, 
    wodurch sich die Code-Balance auf $\frac{3}{2} = 1.5$ reduziert. 
    Um das Modell zu überprüfen, wurden beide Varianten gebenchmarkt. 
    Die Ergebnisse sind in Abbildung 7 dargestellt. Man erkennt, % hier muss ein Quellenverweis hin
    dass die Loop-Fusion-Variante schneller ist als die nicht fusionierte Variante.

\section{Der STREAM Benchmark}
    Der STREAM Benchmark besteht aus vier Kernels, die die Bandbreite des Speicherinterfaces meist in GB/s messen sollen.
    Die vier Kernels sind Copy, Scale, Add und Triad wobei in der Tabelle auch die Codebalance nochmal aufgeführt ist.
    

    \begin{table}[h]
        \centering
        \begin{tabular}{|c|c|c|c|}
        \hline
        \textbf{Kernelname} & \textbf{Operation} & \textbf{Code-Balance} \\
        \hline
        Copy & A[i] = B[i] & - \\
        \hline
        Scale & A[i] = q*B[i] & 2/1 \\
        \hline
        Add & A[i] = B[i] + C[i] & 2/1 \\
        \hline
        Triad & A[i] = B[i] + q*C[i] & 3/2 \\
        \hline
        \end{tabular}
        \caption{STREAM Benchmark Kernels und ihre Code-Balance}
        \label{tab:stream_benchmark}
        \end{table}


    Für die Ausführung des STREAM-Benchmarks ist es wichtig, 
    dass die Datenmengen ausreichend groß sind. AMD empfiehlt, 
    dass die Arrays mindestens viermal so groß sein sollten wie die Summe aller Last-Level-Caches. % Hier muss ein Quellenverweis hin. 
    Im vorherigen Abschnitt wurde der Scale-Kernel genutzt, 
    um die Effektivität der Loop-Fusion zu demonstrieren. 
    Dabei wurde die Zeit jedoch in Nanosekunden anstelle von GB/s gemessen. 
    Da diese synthetischen Kernels die Hardware direkt testen, 
    sind die erreichten Bandbreitenwerte aussagekräftigere Vergleichswerte als die, 
    die man beispielsweise auf den Webseiten von AMD oder Intel findet, 
    um die Leistung von Code und Hardware einzuschätzen. % Hier muss ein Quellenverweis hin.
    Abbildung 8 stellt die Ergebnisse des Triad-Kernels auf einem i7-9700K dar. % Hier muss ein Quellenverweis hin. 
    Es ist erkennbar, dass die Bandbreite mit jedem Cache-Level abnimmt, 
    wobei der Rückgang bei dem L3-Cache besonders stark ist. 
    Derselbe Benchmark wurde auch auf dem ARA-Cluster auf einem der FSU Jena durchgeführt. 
    Die Ergebnisse, die in Abbildung 9 dargestellt sind, verdeutlichen nochmals, 
    wie die Bandbreite von Cache-Level zu Cache-Level abnimmt. % Hier muss ein Quellenverweis hin.
    Sind Ergebnisse eines passenden Tests vorhanden sollte man also die vorhin vorgestellte Formel zur Berechnung der maximalen erreichbaren
    Performance zu $\textnormal{P} =  \min(\textnormal{P}_{\textnormal{max}}, 
    \frac{\textnormal{b}_{\textnormal{Stream}}}{\textnormal{B}_{\textnormal{c}}})$ abändern.

\section{Das Beachten des Speicherzugriffsmusters}
    Bei zwei Dimensionalen Arrays bestimmt die Programmiersprache die Reihenfolge 
    in der die Elemente im Speicher abgelegt werden. Fortran, Julia und R werden beispielsweise
    in Column-Major-Order abgelegt, während C++ in Row-Major-Order abgelegt wird.
    Greift man auf ein Element zu, dann wird die zugehörige Cache Line geladen.
    Im besten Fall werden anschließend alle Elemente der Cache Line abgerufen dann die Cache Line verdrängt 
    und nie wieder benötigt. Im schlechtesten Fall wird die Cache Line verdrängt bevor alle Elemente abgerufen wurden 
    oder sogar nur ein Element genutzt, wodurch der Zweck des Caches verfehlt wird.
    In Abbildung 10 ist ein Beispiel für ein 2D Array in Row-Major-Order dargestellt. % Hier muss ein Quellenverweis!!!!! hin.
    Würde man in dieser Matrix Spaltenweise auf die Elemente zugreifen,
    dann würde jeder Zugriff das Laden einer neuen Cache Line erfordern, was man Strided Zugriff nennt, 
    weil man restlichen Elemente der Zeile überspringt.
    Um die Auswirkungen des Speicherzugriffsmusters zu demonstrieren, sind hier zwei 
    Varianten einer Matrix Vektor Multiplikation dargestellt.

    \begin{minted}{cpp}
    void gemm_row(double *X, double *Y, double *Z, int n)
    {
        for (std::size_t l_m = 0; l_m < n; l_m++)
        {
            for (std::size_t l_k = 0; l_k < n; l_k++)
            {
                for (std::size_t l_n = 0; l_n < n; l_n++)
                {
                    Z[l_m * n + l_n] += X[l_m * n + l_k] * Y[l_k * n + l_n];
                }
            }
        }
    }
    
    void gemm_column(double *X,
                     double *Y,
                     double *Z,
                     int n)
    {
    
        for (std::size_t l_n = 0; l_n < n; l_n++)
        {
            for (std::size_t l_k = 0; l_k < n; l_k++)
            {
                for (std::size_t l_m = 0; l_m < n; l_m++)
                {
                    Z[l_m * n + l_n] += X[l_m * n + l_k] * Y[l_k * n + l_n];
                }
            }
        }
    }
    \end{minted}











% Literaturverzeichnis
\begin{thebibliography}{9}
\bibitem{ref1} Autor1, Titel des Referenzartikels, Journal, Jahr.
\bibitem{ref2} Autor2, Titel des Referenzartikels, Journal, Jahr.
\end{thebibliography}

\end{document}